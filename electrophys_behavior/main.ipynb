{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CA1 neurons during the autopi task\n",
    "\n",
    "The analysis depends on 2 main packages developped in the lab.\n",
    "\n",
    "* spikeA\n",
    "* autopypi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename positrack2 files\n",
    "\n",
    "The positrack2 software has the following name scheme: mouse-date-time.positrack2\n",
    "We want to have mouse-date_01.positrack2, mouse-date_02.positrack2, etc\n",
    "\n",
    "* rename_positrack2_files.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power spectra \n",
    "\n",
    "To make sure that the raw .dat files are not corrupted, I will create a power spectrum with the beginning and end of each .dat files. \n",
    "\n",
    "* power_spectra_inspection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel mapping\n",
    "\n",
    "We test whether the channel mapping makes sense using a correlation matrix of raw signals from different channels. The assumption is that channels in close proximity should be more correlated than further away channels.\n",
    "\n",
    "* channels_correlation_matrix.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database tests\n",
    "\n",
    "Before we start, we want to make sure we have a database with all the needed files.\n",
    "\n",
    "* database_test.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of lever position\n",
    "\n",
    "We need to extract the lever position for further analysis. We can do this with a deeplabcut network applied to the arena video.\n",
    "\n",
    "We use the same procedure as for the behavioral data set.\n",
    "\n",
    "* position_tracking.ipynb\n",
    "\n",
    "It takes approximately 30 minutes per session to perform the analysis with a RTX 2080 ti NVIDIA GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve position tracking using off-line algorithm and additional arena camera\n",
    "\n",
    "The online tracking algorithm is set to minimize false positive at all cost because the data are then passed to the cable actuator. \n",
    "\n",
    "For some sessions with positrack2, we have the recorded video that can be used to improve the tracking that was done online. \n",
    "\n",
    "\n",
    "* dlc_on_positrack2_videos.ipynb -> offline tracking for some sessions with video\n",
    "* kalman_filter.ipynb -> apply kalman filter to remove noise and complete path when small stretches are missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spatial transformation of position data and temporal transformation from ROS time to electrophysiology time.\n",
    "\n",
    "We used 2 different camera systems during the autopi task with electrophysiology. The Positrack software tracked infrared LEDs and we also recorded a video of the arena to know the lever position.\n",
    "\n",
    "We transformed the position data so that these two systems are aligned. \n",
    "\n",
    "The rule is that...\n",
    "\n",
    "* 0,0 is the center of the arena\n",
    "* The home base relative to the center of the arena is in the direction of 0,-1 vector. \n",
    "* The values are in cm.\n",
    "\n",
    "There are also 2 different reference times. The recording times (duration within ktan .dat files) and ROS time (computers clock).\n",
    "\n",
    "To do the analysis, it is simpler if all the data are in the same spatial and temporal reference frame, or if they have both times.\n",
    "\n",
    "The animal pose time is usually using the recording time, we added the ROS time for all position data. We save the ROS time in the `Animal_pose.pose` matrix (7th column).\n",
    "\n",
    "The lever position is also calculated for all row of the `Animal_pose.pose` matrix. This way, we can get the lever position for each trial.\n",
    "\n",
    "For positrack data, we do not see the arena so we will do the detection manually. There are approximately 50 sessions.\n",
    "\n",
    "* positrack_data_transform.ipynb\n",
    "\n",
    "\n",
    "We have position data that were filtered using a Kalman filter in `pose_kf.npy` and unfiltered data in `pose.npy`. The filtered data should be used to analyse the behavior on the autopi task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lever pose file to go with animal pose.\n",
    "\n",
    "* lever_pose.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge position in arena reference frame (cm)\n",
    "\n",
    "We need to get the bridge position in the arena-center reference frame. The coordinates should be in cm\n",
    "\n",
    "We will use `arenaCoordinates` and `bridgeCoordinates`, and generate `bridgeCoordinatesCm`\n",
    "\n",
    "* bridge_coordinates_cm.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check head-direction against heading in the path\n",
    "\n",
    "We want to make sure that 0, pi/2, pi, and -pi/2 means the same thing in all our sessions. For instance, does 0 refer to East, North, West or South? We need to have the same reference frame throughout the experiment. 2 tracking systems were used so it is important to check.\n",
    "\n",
    "* check_head_direction_heading.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify channels with sharp-wave ripples (SWR) and the pyramidal cell layer.\n",
    "\n",
    "SWR are seen only in the pyramidal cell layer of the hippocampus.\n",
    "We can use their presence to identify which shanks of the probes were in the pyramidal cell layer.\n",
    "We detected SWRs from the last rest trial of the session programatically.\n",
    "Then we performed a visual inspection of SWR events to determine which shanks were in the pyramidal cell layer.\n",
    "\n",
    "* sharp_wave_ripples_detection.ipynb\n",
    "\n",
    "We stored the results in `shank_layer.csv` file, whith shankNo and layer as header. This file is later used to select neurons from the pyramidal cell layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mean spike waveforms of clusters\n",
    "\n",
    "This gets the waveform on the channel with the largest amplitude. The channel with the largest amplitude is also saved in order to assign a shank to each neuron.\n",
    "\n",
    "* mean_waveform.ipynb\n",
    "\n",
    "## Identify the shank on which a neuron was recorded\n",
    "\n",
    "* shank_of_neurons.ipynb\n",
    "\n",
    "\n",
    "## Classification as pyramidal cells or interneurons\n",
    "\n",
    "* cell_type_classification2.ipynb\n",
    "\n",
    "This notebook generates 2 supplementary figures and `/adata/projects/autopi_ca1/results/cell_classification.csv` that can be used later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the trial table\n",
    "\n",
    "Create a table containing  autopy trials start and end time and the light condition. The time is in ROS and electrophysiological time. \n",
    "\n",
    "This trial table can be loaded for later analysis without always having to recalculate these.\n",
    "\n",
    "* trial_table_simple.ipynb\n",
    "\n",
    "It is called \"simple\" because the trials are not yet decomposed in journeys.\n",
    "\n",
    "A file called `trial_table_simple` is created in each session directory.\n",
    "\n",
    "More detailed trial extraction below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trials, journeys and NavPath extraction\n",
    "\n",
    "Here we extract trials, journeys and NavPath for each session.\n",
    "\n",
    "We create 2 DataFrames\n",
    "\n",
    "* One summarizing each NavPath of the session\n",
    "* One with the instantaneous behavioral data within each NavPath\n",
    "\n",
    "The strategy for the data analysis is that we will get the instantaneous firing rate of neurons that can be matched to the instantaneous behavioral data (time in path, distance run, x position, y position, etc). \n",
    "\n",
    "We also save the myProject object with the trial extraction done so that we can reload it later without re-running the analysis all the time.\n",
    "\n",
    "* extract_trial.ipynb\n",
    "\n",
    "Output\n",
    "* /adata/projects/autopi_ca1/results/myProjectWithTrials.pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps of open field, light trials and dark trials\n",
    "\n",
    "To get an idea of the firing properties of neurons during light and dark trails, we create firing rate maps of open field, light and dark trials.\n",
    "\n",
    "* **open_field_light_dark_maps.ipynb**\n",
    "\n",
    "Creates\n",
    "\n",
    "* /adata/projects/autopi_ca1/results/myAutos.pickle\n",
    "* /adata/projects/autopi_ca1/results/myMaps.pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare maps between open field, light and dark trials\n",
    "\n",
    "The main aim is to study the difference between the open field and the task, and between light and dark trials. \n",
    "\n",
    "We use standard place cell analysis that most readers will want to see (mean rate, peak rate, info scores, map correlations)\n",
    "\n",
    "We also look at cell pairs IFR correlation and map similarity for different environments.\n",
    "\n",
    "This is not going into the details of trials. \n",
    "\n",
    "* **light_dark_stats.ipynb**\n",
    "\n",
    "Creates\n",
    "\n",
    "* /adata/projects/autopi_ca1/results/mapCorDf.csv\n",
    "* /adata/projects/autopi_ca1/results/myMapStats.csv\n",
    "\n",
    "\n",
    "* **light_dark_stats_pairs.ipynb** (IRF and map similarity of cell pairs)\n",
    "\n",
    "\n",
    "* **NOT USED ANYMORE: light_dark_stats_figure.ipynb** Figure introducing place cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps of search and homing path for light and dark trials\n",
    "\n",
    "To get an idea of the firing properties of neurons during light and dark trails, we create firing rate maps of open field, light and dark trials.\n",
    "\n",
    "* **search_homing_light_dark_maps.ipynb**\n",
    "* **search_homing_stats.ipynb**\n",
    "* **search_homing_stats_pair.ipynb**\n",
    "\n",
    "Creates\n",
    "\n",
    "* /adata/projects/autopi_ca1/results/mySearchHomingMaps.pickle\n",
    "* /adata/projects/autopi_ca1/results/mySearchHomingMapStats.csv\n",
    "* /adata/projects/autopi_ca1/results/searchHomingMapCorDf.csv\n",
    "\n",
    "\n",
    "Analysis with figure\n",
    "* **NOT USED ANYMORE search_homing_figure.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge comparison of random foraging with task and search/homing paths into one figure\n",
    "\n",
    "This uses the data generated in the two sections above and make a figure with it.\n",
    "\n",
    "It is copied from `light_dark_stats_figure.ipynb` and `search_homing_figure.ipynb`\n",
    "\n",
    "* **remapping_figure.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantaneous firing rates\n",
    "\n",
    "We create a matrix containing the instantaneous firing rate of all neurons at 50Hz. The data is stored in a file in each session directory.\n",
    "\n",
    "These will be correlated to the behavioral variables later on.\n",
    "\n",
    "* instantaneous_firing_rate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between instantaneous firing rate and NavPath variables \n",
    "\n",
    "* ifr_behav_exploration.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Homing path analysis\n",
    "\n",
    "We need to find out what control the activity of pyramidal cells when the animal is \"searching\" for the lever.\n",
    "\n",
    "* **search_path_data_collection.ipynb**\n",
    "* **homing_path_data_collection.ipynb**\n",
    "\n",
    "\n",
    "* **search_homing_trial_matrix.ipynb**\n",
    "\n",
    "The notebook above calculate the trial rate matrix internal correlation\n",
    "\n",
    "\n",
    "\n",
    "* search_path_model_fields.ipynb # not used yet in figure\n",
    "* search_path_fast_slow_paths.ipynb # not successful yet\n",
    "\n",
    "Current figure \n",
    "* OLD: search_path_figure_identify_fields.ipynb\n",
    "* **search_homing_path_figure.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lever-centered maps\n",
    "\n",
    "We can check if some neurons have a nice firing field locked on the lever position. We would expect this to be stronger in the light when the animal sees the lever.\n",
    "\n",
    "* lever_centered_maps.ipynb\n",
    "\n",
    "This notebook creates many data files including:\n",
    "\n",
    "\n",
    "* /adata/projects/autopi_ca1/results/myLeverStats.csv\n",
    "* /adata/projects/autopi_ca1/results/myLeverMaps.pickle\n",
    "* /adata/projects/autopi_ca1/results/myLeverStatsThreshold.csv\n",
    "* /adata/projects/autopi_ca1/results/dfLev.csv\n",
    "\n",
    "The following notebook create figures \n",
    "\n",
    "* lever_centered_maps_figure.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lever-centered maps related to search path and homing error\n",
    "\n",
    "We have firing feilds around the lever that are lock to the cardinal reference frame. \n",
    "\n",
    "Are these fields related to the properties of the search path?\n",
    "Are these fields related to homing accuracy?\n",
    "\n",
    "We have one jupyter notebook to generate the maps we need.\n",
    " * **lever_centered_sh.ipynb** \n",
    "\n",
    "And one jupyter notebook to do stats and figures.\n",
    "* **lever_centered_sh_figure.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head direction selectivity\n",
    "\n",
    "We want to compare head-direction coding during open field, light and dark condition.\n",
    "\n",
    "* open_field_light_dark_hd_histo.ipynb\n",
    "\n",
    "This notebook calculates all the HD firing rate histograms (and respective firing rate maps). We also get the stats related to HD coding. \n",
    "The results are saved in \n",
    "\n",
    "* /adata/projects/autopi_ca1/results/myDirMaps.pickle, dictionary with key per condition\n",
    "* /adata/projects/autopi_ca1/results/dirMapCorDf.csv, data frame with correlation of north and south maps\n",
    "* /adata/projects/autopi_ca1/results/myHdHist.pickle, dictionary with key per condition\n",
    "* /adata/projects/autopi_ca1/results/myHdStats.csv, data frame with hd stats\n",
    "* /adata/projects/autopi_ca1/results/myHdStability.csv, data frame with stability (r) for across different conditions (oo,ol,od,ld)\n",
    "* /adata/projects/autopi_ca1/results/myHdThreshold.csv, threshold for the vector length\n",
    "\n",
    "\n",
    "There are some figures in the notebook to test the sanity of the analysis.\n",
    "\n",
    "* light_dark_hd_figure.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homing vector coding\n",
    "\n",
    "Test whether some hippocampal neurons encode information the difference between heading and homing direction. \n",
    "\n",
    "* homing_vector_histo.ipynb\n",
    "\n",
    "This notebook calculate egocentric homing angle firing rate histograms and the stats of these histograms\n",
    "\n",
    "* /results/myEhaHist.pickle\n",
    "* /results/myEhaStats.csv\n",
    "\n",
    "\n",
    "We are comparing HD and EHA coding in the following notebook\n",
    "\n",
    "* homing_vector_histo_figure.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the path on the mouse for each trial of a session\n",
    "\n",
    "We try to plot the path of the mouse for each trial on the task.\n",
    "\n",
    "* plot_single_trial_path.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
